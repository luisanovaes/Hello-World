{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuxBUqvLtv7pFrYOXGA0KJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisanovaes/ufam-project/blob/master/Code_summarization_coliee-sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWIAii6PcaNr"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "#Acesso ao drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Drqace_he7N",
        "outputId": "f3045efb-8fc3-445a-983c-f19960f017b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_data(drive_folder_path):\n",
        "  documents = []\n",
        "  list_of_ids = []\n",
        "  i=0\n",
        "  for txt in glob(os.path.join(drive_folder_path, '*.txt')):\n",
        "    with open(txt, encoding='utf-8') as tfile:\n",
        "      text = tfile.read()\n",
        "      text = text.replace('\\n',' ').replace('FRAGMENT_SUPPRESSED','')\n",
        "      documents.append(text)\n",
        "      list_of_ids.append(txt)\n",
        "      list_of_ids[:] = [d.replace(drive_folder_path,'') for d in list_of_ids]\n",
        "  return documents, list_of_ids\n",
        "\n",
        "def get_segments(document):\n",
        "  # Get segments from txt by splitting on .\n",
        "  segments =  document.split('.')\n",
        "  # Put the . back in\n",
        "  segments = [segment + '.' for segment in segments]\n",
        "  # Further split by comma\n",
        "  segments = [segment.split(',') for segment in segments]\n",
        "  # Flatten\n",
        "  segments = [item for sublist in segments for item in sublist]\n",
        "  return segments\n",
        "\n",
        "\n",
        "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
        "\n",
        "  # Combine the non-sentences together\n",
        "  sentences = []\n",
        "\n",
        "  is_new_sentence = True\n",
        "  sentence_length = 0\n",
        "  sentence_num = 0\n",
        "  sentence_segments = []\n",
        "\n",
        "  for i in range(len(segments)):\n",
        "    if is_new_sentence == True:\n",
        "      is_new_sentence = False\n",
        "    # Append the segment\n",
        "    sentence_segments.append(segments[i])\n",
        "    segment_words = segments[i].split(' ')\n",
        "    sentence_length += len(segment_words)\n",
        "\n",
        "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
        "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
        "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
        "      sentence = ' '.join(sentence_segments)\n",
        "      sentences.append({\n",
        "        'sentence_num': sentence_num,\n",
        "        'text': sentence,\n",
        "        'sentence_length': sentence_length\n",
        "      })\n",
        "      # Reset\n",
        "      is_new_sentence = True\n",
        "      sentence_length = 0\n",
        "      sentence_segments = []\n",
        "      sentence_num += 1\n",
        "\n",
        "  return sentences\n",
        "\n",
        "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
        "\n",
        "  sentences_df = pd.DataFrame(sentences)\n",
        "\n",
        "  chunks = []\n",
        "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
        "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
        "    chunk_text = ' '.join(chunk['text'].tolist())\n",
        "\n",
        "    chunks.append({\n",
        "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
        "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
        "      'text': chunk_text,\n",
        "      'num_words': len(chunk_text.split(' '))\n",
        "    })\n",
        "\n",
        "  chunks_df = pd.DataFrame(chunks)\n",
        "  return chunks_df.to_dict('records')\n"
      ],
      "metadata": {
        "id": "ufzNERpiIqm8"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentences2(segments, MIN_WORDS, MAX_WORDS):\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    is_new_sentence = True\n",
        "    sentence_length = 0\n",
        "    sentence_num = 0\n",
        "    sentence_segments = []\n",
        "\n",
        "    for i in range(len(segments)):\n",
        "        if is_new_sentence:\n",
        "            is_new_sentence = False\n",
        "        # Append the segment\n",
        "        sentence_segments.append(segments[i])\n",
        "        segment_words = segments[i].split(' ')\n",
        "        sentence_length += len(segment_words)\n",
        "\n",
        "        # If exceed MAX_WORDS, then stop at the end of the segment\n",
        "        # Only consider it a sentence if the length is at least MIN_WORDS\n",
        "        if (sentence_length >= MIN_WORDS and segments[i] and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
        "            sentence = ' '.join(sentence_segments)\n",
        "            sentences.append({\n",
        "                'sentence_num': sentence_num,\n",
        "                'text': sentence,\n",
        "                'sentence_length': sentence_length\n",
        "            })\n",
        "            # Reset\n",
        "            is_new_sentence = True\n",
        "            sentence_length = 0\n",
        "            sentence_segments = []\n",
        "            sentence_num += 1\n",
        "\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "RGjFtU40Xy7x"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code to import documents"
      ],
      "metadata": {
        "id": "q7_Q4b4Bw0q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob2 import glob\n",
        "# Caminho para a pasta do Google Drive montada no Colab\n",
        "drive_folder_path = '/content/drive/MyDrive/COLIEE-2023/Amostra-COLIEE/'  # Substitua 'sua_pasta' pelo nome da sua pasta no Google Drive\n",
        "\n",
        "\n",
        "#load data (documents and lista of ids)\n",
        "documents, list_of_id = load_data(drive_folder_path)\n",
        "#remove blank space from id's name\n",
        "list_of_ids = [item.replace(' ', '') for item in list_of_id]\n",
        "\n",
        "\n",
        "#create a dictionary document->id document\n",
        "id_to_doc = dict(zip(list_of_ids,documents))\n",
        "doc_to_id = dict(zip(documents,list_of_ids))"
      ],
      "metadata": {
        "id": "FE0c2LHQwvML"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mypath = '/content/drive/MyDrive/099814.txt'\n",
        "#text = \"\"\n",
        "\n",
        "#with open(mypath, encoding='utf-8') as tfile:\n",
        "#    txt099814 = tfile.read()\n",
        "#    txt099814 = txt099814.replace('\\n', ' ').replace('FRAGMENT_SUPPRESSED', '')\n",
        "\n",
        "#segments = get_segments(txt099814)"
      ],
      "metadata": {
        "id": "b3HAjRL5nWEF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install colabtools"
      ],
      "metadata": {
        "id": "IT23lMLa9hrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdb8f88-5d95-411f-df87-bb95b2df1a1b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting colabtools\n",
            "  Downloading colabtools-0.0.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: colabtools\n",
            "Successfully installed colabtools-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import sentencepiece\n",
        "import IPython.display as display\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\")"
      ],
      "metadata": {
        "id": "lgnQeVSg9iuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_chunk={}\n",
        "id_to_encoded_chunk = {}\n",
        "for document in documents:\n",
        "  segment = get_segments(document)\n",
        "  sentences = create_sentences(segment, MIN_WORDS=3, MAX_WORDS=80)\n",
        "  chunks = create_chunks(sentences, CHUNK_LENGTH=19, STRIDE=1)\n",
        "  chunks_text = [chunk['text'] for chunk in chunks]\n",
        "  id_to_chunk[doc_to_id[document]] = chunks_text\n",
        "  # Tokenize each chunk.\n",
        "  encoded_chunks = []\n",
        "  for chunk in chunks:\n",
        "    encoded_chunk = tokenizer.encode(chunk, return_tensors='pt', max_length=1024, truncation=True)\n",
        "    encoded_chunks.append(encoded_chunk)\n",
        "  id_to_encoded_chunk[doc_to_id[document]] = encoded_chunks\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ks1VVc4pnviD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1: Getting Chunk Summaries"
      ],
      "metadata": {
        "id": "6ZqV7K31tlf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Read all docs from a paste, 2) break in chunks, --OK\n",
        "#3) generate a summary for each chunk\n",
        "#4) concatenate all summaries and save in a paste\n",
        "#5) create a summary from the concatenation of all summaries and save in another paste\n"
      ],
      "metadata": {
        "id": "NFrW_bdvtklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Topification of summaries"
      ],
      "metadata": {
        "id": "Q_AoaZeY4Lfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a summary for each chunk.\n",
        "summaries = []\n",
        "for chunk in encoded_chunks:\n",
        "    summary_ids = model.generate(chunk,\n",
        "                                 num_beams=9,\n",
        "                                 no_repeat_ngram_size=3,\n",
        "                                 length_penalty=2.0,\n",
        "                                 min_length=50,\n",
        "                                 max_length=250,\n",
        "                                 early_stopping=True)\n",
        "\n",
        "    # Decode each summary chunk\n",
        "    summary = ' '.join(tokenizer.decode(token_id) for token_id in summary_ids.squeeze())\n",
        "    summaries.append(summary)\n",
        "\n",
        "# Concatenate the summaries of each chunk.\n",
        "summary = ' '.join(summaries)\n",
        "\n",
        "#Save summary"
      ],
      "metadata": {
        "id": "ndQePR_l9m9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate a summary for each chunk.\n",
        "\n",
        "summaries = []\n",
        "for chunk in encoded_chunks:\n",
        "    summary_ids = model.generate(chunk,\n",
        "                                 num_beams=9,\n",
        "                                 no_repeat_ngram_size=3,\n",
        "                                 length_penalty=2.0,\n",
        "                                 min_length=50,\n",
        "                                 max_length=250,\n",
        "                                 early_stopping=True)\n",
        "\n",
        "    # Decode each summary chunk\n",
        "    summary = ' '.join(tokenizer.decode(token_id) for token_id in summary_ids.squeeze())\n",
        "    summaries.append(summary)\n",
        "\n",
        "# Concatenate the summaries of each chunk.\n",
        "summary = ' '.join(summaries)"
      ],
      "metadata": {
        "id": "orFZiA_V9w6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
